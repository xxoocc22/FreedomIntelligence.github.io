---
---

@article{wang2021pre,
  title={Pre-trained Language Models in Biomedical Domain: A Survey from Multiscale Perspective},
  author={Wang, Benyou and Xie, Qianqian and Pei, Jiahuan and Tiwari, Prayag and Li, Zhao and Jie, Fu},
  journal={ACM Computing Surveys},
  year={2023}
}

@inproceedings{wang2022exploring,
  title={Exploring extreme parameter compression for pre-trained language models},
  author={Wang, Benyou and Ren, Yuxin and Shang, Lifeng and Jiang, Xin and Liu, Qun},
  booktitle={ICLR 2022},
  year={2022}
}


@article{tang2022dptdr,
  title={DPTDR: Deep Prompt Tuning for Dense Passage Retrieval},
  author={Tang, Zhengyang and Wang, Benyou and Yao, Ting},
  journal={COLING 2022},
  year={2022},
  arxiv={2208.11503},
  code={https://github.com/FreedomIntelligence/DPTDR}
  
}

@article{yang2022doge,
  title={Doge Tickets: Uncovering Domain-general Language Models by Playing Lottery Tickets},
  author={Yang, Yi and Zhang, Chen and Wang, Benyou and Song, Dawei},
  journal={NLPCC 2022 Best Paper},
  year={2022},
  arxiv={2207.09638},
  code={https://github.com/Ylily1015/DogeTickets}
}

@article{wang2023can,
  title={Can Language Models Make Fun? A Case Study in Chinese Comical Crosstalk},
  author={Wang, Benyou and Wu, Xiangbo and Liu, Xiaokang and Li, Jianquan and Tiwari, Prayag and Xie, Qianqian},
  journal={ACL 2023},
  year={2023},
  arxiv={2207.00735},
  code={https://github.com/FreedomIntelligence/crosstalk-generation}
}

@article{gan2022morphte,
  title={MorphTE: Injecting Morphology in Tensorized Embeddings},
  author={Gan, Guobing and Zhang, Peng and Li, Sunzhu and Lu, Xiuqing and Wang, Benyou},
  journal={NeurIPS 2022},
  year={2022},
  arxiv={2210.15379},
  code={https://github.com/bigganbing/Fairseq_MorphTE}
   
}

@article{li2023adapting,
  title={Adapting Pre-trained Language Models for Quantum Natural Language Processing},
  author={Li, Qiuchi and Wang, Benyou and Zhu, Yudong and Lioma, Christina and Liu, Qun},
  journal={arXiv preprint arXiv:2302.13812},
  year={2023},
  arxiv={2302.13812}
}

@article{han2022document,
  title={Document-level Relation Extraction with Relation Correlations},
  author={Han, Ridong and Peng, Tao and Wang, Benyou and Liu, Lu and Wan, Xiang},
  journal={arXiv preprint arXiv:2212.10171},
  year={2022},
  arxiv={2212.10171}
}

@inproceedings{li2022hypoformer,
  title={Hypoformer: Hybrid decomposition transformer for edge-friendly neural machine translation},
  author={Li, Sunzhu and Zhang, Peng and Gan, Guobing and Lv, Xiuqing and Wang, Benyou and Wei, Junqiu and Jiang, Xin},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={7056--7068},
  year={2022}
}

@article{chen2023towards,
  title={Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts},
  author={Chen, Zhihong and Diao, Shizhe and Wang, Benyou and Li, Guanbin and Wan, Xiang},
  journal={ICCV 2023},
  year={2023},
  arxiv={2302.08958},
  code={https://github.com/zhjohnchan/PTUnifier}  
}

@article{liang2023modular,
  title={Modular Retrieval for Generalization and Interpretation},
  author={Liang, Juhao and Zhang, Chen and Tang, Zhengyang and Fu, Jie and Song, Dawei and Wang, Benyou},
  journal={arXiv preprint arXiv:2303.13419},
  year={2023},
  arxiv={2303.13419},
  code={https://github.com/FreedomIntelligence/REMOP}
}



@inproceedings{liu2023effective,
  title={Effective Open Intent Classification with K-center Contrastive Learning and Adjustable Decision Boundary},
  author={Liu, Xiaokang and Li, Jianquan and Mu, Jingjing and Yang, Min and Xu, Ruifeng and Wang, Benyou},
  booktitle={AAAI},
  year={2023}
}

@article{chen2023phoenix,
  title={Phoenix: Democratizing chatgpt across languages},
  author={Chen, Zhihong and Jiang, Feng and Chen, Junying and Wang, Tiannan and Yu, Fei and Chen, Guiming and Zhang, Hongbo and Liang, Juhao and Zhang, Chen and Zhang, Zhiyi and others},
  journal={arXiv preprint arXiv:2304.10453},
  year={2023},
  arxiv={2304.10453},
  code={https://github.com/FreedomIntelligence/LLMZoo}
}

@article{zhang2023lifting,
  title={Lifting the Curse of Capacity Gap in Distilling Language Models},
  author={Zhang, Chen and Yang, Yang and Liu, Jiahao and Wang, Jingang and Xian, Yunsen and Wang, Benyou and Song, Dawei},
  journal={ACL 2023},
  year={2023}
}

@article{liu2023one,
  title={One Cannot Stand for Everyone! Leveraging Multiple User Simulators to train Task-oriented Dialogue Systems},
  author={LIU, Yajiao and Jiang, Xin and Yin, Yichun and Wang, Yasheng and Mi, Fei and Liu, Qun and Wan, Xiang and Wang, Benyou},
  journal={ACL 2023},
  year={2023}
}

@article{chen2023difference,
  title={On the Difference of BERT-style and CLIP-style Text Encoders},
  author={Chen, Zhihong and Chen, Guiming Hardy and Diao, Shizhe and Wan, Xiang and Wang, Benyou},
  journal={ACL 2023 findings},
  year={2023}
}

@article{sun2023few,
  title={Few-Shot Class-Incremental Learning for Medical Time Series Classification},
  author={Sun, Le and Zhang, Mingyang and Wang, Benyou and Tiwari, Prayag},
  journal={IEEE Journal of Biomedical and Health Informatics},
  year={2023},
  publisher={IEEE}
}

@article{yu2023nature,
  title={Natural language reasoning, a survey},
  author={Yu, Fei and Zhang, Hongbo and Wang, Benyou},
  journal={arXiv preprint arXiv:2303.14725},
  year={2023}
}

@article{li2023huatuo,
  title={Huatuo-26M, a Large-scale Chinese Medical QA Dataset},
  author={Li, Jianquan and Wang, Xidong and Wu, Xiangbo and Zhang, Zhiyi and Xu, Xiaolong and Fu, Jie and Tiwari, Prayag and Wan, Xiang and Wang, Benyou},
  journal={arXiv preprint arXiv:2305.01526},
  year={2023},
  arxiv={2305.01526},
  code={https://github.com/FreedomIntelligence/Huatuo-26M}
}

@article{han2023information,
  title={Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors},
  author={Han, Ridong and Peng, Tao and Yang, Chaohao and Wang, Benyou and Liu, Lu and Wan, Xiang},
  journal={arXiv preprint arXiv:2305.14450},
  year={2023},
  arxiv={2305.14450},
  code={https://github.com/FreedomIntelligence/Evaluation-of-ChatGPT-on-Information-Extraction},
  
}

@article{zhang2023huatuogpt,
  title={HuatuoGPT, towards Taming Language Model to Be a Doctor},
  author={Zhang, Hongbo and Chen, Junying and Jiang, Feng and Yu, Fei and Chen, Zhihong and Li, Jianquan and Chen, Guiming and Wu, Xiangbo and Zhang, Zhiyi and Xiao, Qingying and others},
  journal={arXiv preprint arXiv:2305.15075},
  year={2023},
  arxiv={2305.15075},
  code={https://github.com/FreedomIntelligence/HuatuoGPT}
}

@article{zhang2023injecting,
  title={Injecting Knowledge into Biomedical Pre-trained Models via Polymorphism and Synonymous Substitution},
  author={Zhang, Hongbo and Wan, Xiang and Wang, Benyou},
  journal={arXiv preprint arXiv:2305.15010},
  year={2023}
}

@article{wang2023cmb,
  title={CMB: A Comprehensive Medical Benchmark in Chinese},
  author={Wang, Xidong and Chen, Guiming Hardy and Song, Dingjie and Zhang, Zhiyi and Chen, Zhihong and Xiao, Qingying and Jiang, Feng and Li, Jianquan and Wan, Xiang and Wang, Benyou and others},
  journal={arXiv preprint arXiv:2308.08833},
  year={2023},
  arxiv={2308.08833},
  code={https://github.com/FreedomIntelligence/CMB}
}

@article{kong2023large,
  title={Large Language Model as a User Simulator},
  author={Kong, Chuyi and Fan, Yaxin and Wan, Xiang and Jiang, Feng and Wang, Benyou},
  journal={arXiv preprint arXiv:2308.11534},
  year={2023},
  arxiv={2308.11534},
  code={https://github.com/FreedomIntelligence/ReaLM}
}
